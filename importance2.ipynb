{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19337dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import _pickle as pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold   # 补\n",
    "\n",
    "'''\n",
    "    在这里修改路径\n",
    "    👇👇这一整段都可以自行修改\n",
    "'''\n",
    "part = 'Part2'\n",
    "train_path_list = ['feature\\\\train_feature\\Part2\\split_by_feat\\df_train_ordered_1.parquet']          # 需要加路径\n",
    "\n",
    "label_path = 'feature\\\\train_feature\\labels\\labels.parquet'  # label路径\n",
    "\n",
    "model_path = 'model/'            # 需要创建名为{model_path}的文件夹\n",
    "if not os.path.isdir(model_path):\n",
    "    os.makedirs(os.path.join(model_path, part))\n",
    "\n",
    "id_path = 'id/'            # 需要创建名为{id_path}的文件夹\n",
    "if not os.path.isdir(id_path):\n",
    "    os.makedirs(os.path.join(id_path, part))\n",
    "\n",
    "importance_path = 'importance/'            # 需要创建名为{importance_path}的文件夹\n",
    "if not os.path.isdir(importance_path):\n",
    "    os.makedirs(os.path.join(importance_path, part))\n",
    "\n",
    "label_feat = ['label_5','label_10','label_20', 'label_40', 'label_60']\n",
    "\n",
    "\n",
    "\n",
    "def saveDict(data, path):\n",
    "    pickle.dump(data, open(path, 'wb'), protocol=4)\n",
    "\n",
    "def fromDict(path):\n",
    "    print('loading file:', path, end=' ')\n",
    "    t = time.time()\n",
    "    data = pickle.loads(open(path,'rb').read())\n",
    "    print('Time spent:', time.time() - t)\n",
    "    return data\n",
    "\n",
    "def load_data_list(path_list):\n",
    "    datas = []\n",
    "    for path in tqdm(path_list):\n",
    "        datas.append(pd.read_parquet(path))\n",
    "    return pd.concat(datas, axis=0)\n",
    "\n",
    "def get_timestamp(data):\n",
    "    h, m, s = data['time'].split(':')\n",
    "    h, m, s = int(h), int(m), int(s)\n",
    "    timestamp = h * 3600 + m * 60 + s\n",
    "    return timestamp\n",
    "    \n",
    "\n",
    "def load_data(path):\n",
    "    print('reading', path, '...')\n",
    "    data = pd.read_parquet(path)\n",
    "    print('End reading')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f158c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "callbacks = [log_evaluation(period=10), early_stopping(stopping_rounds=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0ff104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = y_hat.reshape(3,-1).T  # 3 分类\n",
    "    y_hat = np.argmax(y_hat, axis=-1)\n",
    "    return 'f1', f1_score(y_true, y_hat, average='macro'), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0cdf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading feature\\train_feature\\Part2\\split_by_feat\\df_train_ordered_1.parquet ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for index, path in enumerate(train_path_list):\n",
    "    X = load_data(path)\n",
    "    \n",
    "    X = X.iloc[:100]\n",
    "\n",
    "\n",
    "    \n",
    "    file_name = path.split('\\\\')[-1].split('.')[0]\n",
    "    print(file_name)\n",
    "    print(X)\n",
    "    # input_data['time_stamp'] = input_data.apply(get_timestamp, axis=1)\n",
    "\n",
    "    Y = pd.read_parquet(label_path).set_index(['date', 'time', 'sym'])\n",
    "    Y = Y.reindex(X.index)\n",
    "\n",
    "\n",
    "    # 定义LightGBM模型参数\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'metric': 'multi_logloss',\n",
    "        # 'metric': 'custom',  # 设置为'custom'，因为我们使用自定义的评估函数\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        # 'device': 'gpu',\n",
    "        # 'gpu_platform_id': 0,\n",
    "        # 'gpu_devices_id': 0\n",
    "    }\n",
    "\n",
    "    n_fold = 3            # 补\n",
    "    ids_folds = {}          # 补\n",
    "\n",
    "    # 对每个标签单独训练一个模型\n",
    "    models = {}\n",
    "    for i, label in enumerate(label_feat):\n",
    "        print('Begin training', label, '...')\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=n_fold)          # 补\n",
    "        importance_df = None          # 补\n",
    "\n",
    "        for fold, (idx_tr, idx_va) in enumerate(kf.split(X, Y.iloc[:, 0])):          # 不能用multi-label\n",
    "            X_train = X.iloc[idx_tr]          # 补\n",
    "            X_val = X.iloc[idx_va]          # 补\n",
    "            Y_train = Y.iloc[idx_tr]          # 补\n",
    "            Y_val = Y.iloc[idx_va]          # 补\n",
    "\n",
    "            ids_folds[(label, fold)] = (idx_tr, idx_va)          # 补\n",
    "\n",
    "            y_train = Y_train[label]\n",
    "            y_val = Y_val[label]\n",
    "\n",
    "            # 创建LightGBM数据集\n",
    "            train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "            val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "            # 训练模型\n",
    "            model = lgb.train(params, train_dataset, valid_sets=[train_dataset, val_dataset], num_boost_round=10000, callbacks=callbacks, feval=lgb_f1_score)\n",
    "\n",
    "            importance = model.feature_importance(importance_type='split')\n",
    "            feature_name = model.feature_name()\n",
    "            \n",
    "            \n",
    "            # 补\n",
    "            if importance_df is None:\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': feature_name,\n",
    "                    'Importance': importance\n",
    "                })\n",
    "            else:\n",
    "                importance_df['Importance'] += importance\n",
    "            print(importance_df)\n",
    "\n",
    "            models[(fold, label)] = model\n",
    "            print('End training', label, '...\\n\\n')\n",
    "            del X_train, X_val, Y_train, Y_val\n",
    "            gc.collect()\n",
    "\n",
    "        # 补\n",
    "        # 求均值，并保存特征重要性\n",
    "        importance_df['Importance'] /= n_fold\n",
    "        importance_path = os.path.join(importance_path, part, f'{file_name}_{label}_fold{n_fold}.csv')\n",
    "        print('saving importance ...')\n",
    "        importance_df.to_csv(importance_path, index=False)\n",
    "        del importance_df\n",
    "        gc.collect()\n",
    "\n",
    "    del X, Y\n",
    "    gc.collect()\n",
    "\n",
    "    print('saving model ...')\n",
    "    saveDict(models, os.path.join(model_path, part, f'Model_{file_name}_{label}.pkl'))\n",
    "    print('saving id_fold ...')\n",
    "    id_fold_path = os.path.join(id_path, part, f'ID_{file_name}.pkl')\n",
    "    saveDict(ids_folds, id_fold_path)\n",
    "    del models, ids_folds\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828735f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
